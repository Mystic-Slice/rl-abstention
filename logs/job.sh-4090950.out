==========================================
SLURM_JOB_ID = 4090950
SLURM_JOB_NODELIST = a02-15
TMPDIR = /tmp/SLURM_4090950
==========================================
INFO 10-07 14:43:58 [__init__.py:220] No platform detected, vLLM is running on UnspecifiedPlatform
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:32<01:04, 32.36s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:04<00:32, 32.47s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:05<00:00, 18.01s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:05<00:00, 21.91s/it]
trainable params: 33,030,144 || all params: 4,055,498,240 || trainable%: 0.8145
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): Qwen3ForCausalLM(
      (model): Qwen3Model(
        (embed_tokens): Embedding(151936, 2560)
        (layers): ModuleList(
          (0-35): 36 x Qwen3DecoderLayer(
            (self_attn): Qwen3Attention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=2560, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear(
                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=2560, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=2560, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=2560, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
            )
            (mlp): Qwen3MLP(
              (gate_proj): lora.Linear(
                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=2560, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=9728, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear(
                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=2560, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=9728, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear(
                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=9728, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=2560, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)
            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)
          )
        )
        (norm): Qwen3RMSNorm((2560,), eps=1e-06)
        (rotary_emb): Qwen3RotaryEmbedding()
      )
      (lm_head): Linear(in_features=2560, out_features=151936, bias=False)
    )
  )
)
Qwen2TokenizerFast(name_or_path='Qwen/Qwen3-4B-Instruct-2507', vocab_size=151643, model_max_length=1010000, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151665: AddedToken("<tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151666: AddedToken("</tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151667: AddedToken("<think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151668: AddedToken("</think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
Device:  cpu
Generating train split:   0%|          | 0/182822 [00:00<?, ? examples/s]Generating train split:   1%|          | 1000/182822 [00:00<01:19, 2277.21 examples/s]Generating train split:  11%|█         | 20000/182822 [00:00<00:03, 46789.48 examples/s]Generating train split:  33%|███▎      | 61000/182822 [00:00<00:00, 138097.32 examples/s]Generating train split:  59%|█████▉    | 108000/182822 [00:01<00:00, 127666.07 examples/s]Generating train split:  78%|███████▊  | 143000/182822 [00:01<00:00, 139544.10 examples/s]Generating train split: 100%|██████████| 182822/182822 [00:02<00:00, 74961.36 examples/s] 
Generating test split:   0%|          | 0/6150 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 6150/6150 [00:00<00:00, 313521.36 examples/s]
Generating validation split:   0%|          | 0/4183 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 4183/4183 [00:00<00:00, 151793.72 examples/s]
Map (num_proc=8):   0%|          | 0/182822 [00:00<?, ? examples/s]Map (num_proc=8):   0%|          | 486/182822 [00:02<14:05, 215.53 examples/s]Map (num_proc=8):   4%|▍         | 7508/182822 [00:02<00:40, 4359.67 examples/s]Map (num_proc=8):   6%|▌         | 11315/182822 [00:02<00:25, 6693.27 examples/s]Map (num_proc=8):  10%|▉         | 17874/182822 [00:02<00:13, 12371.17 examples/s]Map (num_proc=8):  13%|█▎        | 24565/182822 [00:02<00:08, 18826.81 examples/s]Map (num_proc=8):  17%|█▋        | 31927/182822 [00:02<00:05, 26735.39 examples/s]Map (num_proc=8):  22%|██▏       | 39446/182822 [00:02<00:04, 34839.37 examples/s]Map (num_proc=8):  26%|██▌       | 47314/182822 [00:03<00:03, 42290.64 examples/s]Map (num_proc=8):  30%|███       | 55353/182822 [00:03<00:02, 49684.63 examples/s]Map (num_proc=8):  34%|███▍      | 62773/182822 [00:03<00:02, 54303.16 examples/s]Map (num_proc=8):  39%|███▊      | 70792/182822 [00:03<00:01, 58449.60 examples/s]Map (num_proc=8):  43%|████▎     | 77905/182822 [00:03<00:01, 60468.68 examples/s]Map (num_proc=8):  47%|████▋     | 85272/182822 [00:03<00:01, 62685.39 examples/s]Map (num_proc=8):  51%|█████     | 93070/182822 [00:03<00:01, 66715.73 examples/s]Map (num_proc=8):  55%|█████▍    | 100213/182822 [00:03<00:01, 53374.22 examples/s]Map (num_proc=8):  58%|█████▊    | 106252/182822 [00:04<00:02, 32113.91 examples/s]Map (num_proc=8):  62%|██████▏   | 113639/182822 [00:04<00:01, 38530.90 examples/s]Map (num_proc=8):  66%|██████▌   | 120842/182822 [00:04<00:01, 44251.12 examples/s]Map (num_proc=8):  70%|███████   | 128277/182822 [00:04<00:01, 48917.96 examples/s]Map (num_proc=8):  74%|███████▍  | 136029/182822 [00:04<00:00, 53357.68 examples/s]Map (num_proc=8):  79%|███████▉  | 143996/182822 [00:04<00:00, 58997.45 examples/s]Map (num_proc=8):  83%|████████▎ | 151825/182822 [00:04<00:00, 63416.83 examples/s]Map (num_proc=8):  87%|████████▋ | 158788/182822 [00:05<00:00, 64577.42 examples/s]Map (num_proc=8):  91%|█████████ | 165729/182822 [00:05<00:00, 64550.05 examples/s]Map (num_proc=8):  95%|█████████▍| 173197/182822 [00:05<00:00, 64832.90 examples/s]Map (num_proc=8):  99%|█████████▊| 180374/182822 [00:05<00:00, 53388.91 examples/s]Map (num_proc=8): 100%|██████████| 182822/182822 [00:09<00:00, 18764.83 examples/s]
DatasetDict({
    train: Dataset({
        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name', 'prompt', 'correct_option', 'idk_option'],
        num_rows: 182639
    })
    test: Dataset({
        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name', 'prompt', 'correct_option', 'idk_option'],
        num_rows: 183
    })
})
Sample prompt:
[{'content': "Answer the following question. Provide your thoughts between <reasoning> and </reasoning> symbols. Provide the final answer option (letter only) between <answer> and </answer> symbols. Answer only if you are certain, else choose I Don't Know.Question: Among Bipyridyl herbicides, 1) Diquat is only half as toxic as paraquat 2) Diquat doesnot produce pulmonary toxicity\nOptions: \nA: Both are true\nB: Both are false\nC: 1 is true, 2 is false\nD: 2 is true, 1 is false\nE: I Don't Know", 'role': 'user'}]
A
E
Traceback (most recent call last):
  File "/home1/am59710/rl-abstention/train.py", line 30, in <module>
    training_args = GRPOConfig(
                    ^^^^^^^^^^^
  File "<string>", line 182, in __init__
  File "/home1/am59710/.local/lib/python3.11/site-packages/trl/trainer/grpo_config.py", line 642, in __post_init__
    super().__post_init__()
  File "/home1/am59710/.local/lib/python3.11/site-packages/transformers/training_args.py", line 1739, in __post_init__
    raise ValueError(error_message)
ValueError: Your setup doesn't support bf16/gpu.
